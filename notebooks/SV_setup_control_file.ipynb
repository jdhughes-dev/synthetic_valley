{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Build the pest interface, generate the Prior, and set observation values, weights, and noise\n",
    "\n",
    "This notebook gets us ready to run pest++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import shutil\n",
    "import psutil\n",
    "import numpy as np\n",
    "import flopy\n",
    "import pandas as pd\n",
    "import pyemu\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = helpers.get_domain_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Choose your original set of model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_org_d = os.path.join(\"..\", \"models\", \"synthetic-valley-base-annual-optwell\")\n",
    "# safe_org_d = os.path.join(\"..\", \"models\", \"synthetic-valley-working-advanced-annual\")\n",
    "assert os.path.exists(safe_org_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Define the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_d = \"model_and_pest_files\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "If you want to run a prior monte carlo at the end of this notebook, and, if so, how many worker to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prior_mc = False\n",
    "num_workers = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = psutil.cpu_count(logical=False)\n",
    "\"this computer has {0} cores\".format(cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "And choose the length of the window (in months) to use for smoothing the observed timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 18  # months"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Make a copy of the safe set of model files and run mf6 in that directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_d = \"temp\"\n",
    "if os.path.exists(tmp_d):\n",
    "    shutil.rmtree(tmp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Load the simulation from the original location, and re-write it in the temp directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(sim_ws=safe_org_d)\n",
    "sim.set_sim_path(tmp_d)\n",
    "gwf = sim.get_model()\n",
    "gwf.set_all_data_external(external_data_folder=\".\")\n",
    "sim.write_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Sometimes, you can get by being a lil bit \"lose and fast\" with the solver settings if you dont need high-fidelty derivatives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.remove_package(\"ims\")\n",
    "ims = flopy.mf6.ModflowIms(\n",
    "    sim,\n",
    "    print_option=\"summary\",\n",
    "    complexity=\"complex\",\n",
    "    under_relaxation=None,\n",
    "    linear_acceleration=\"bicgstab\",\n",
    "    outer_maximum=500,\n",
    "    inner_maximum=100,\n",
    "    outer_dvclose=1e-3,\n",
    "    inner_dvclose=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Usually a good idea to let the model keep running even if it fails to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.continue_ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.write_simulation()\n",
    "sim.run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Make a `PstFrom` instance.  This will be how we build up the pest interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pyemu.utils.PstFrom(\n",
    "    tmp_d,\n",
    "    working_d,\n",
    "    remove_existing=True,\n",
    "    spatial_reference=gwf.modelgrid,\n",
    "    zero_based=False,\n",
    "    start_datetime=gwf.start_datetime,\n",
    "    echo=False,\n",
    "    chunk_len=1000000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We are using a model-post-processing function clean up and process csv output files.  We need to tell `PstFrom` to run that function after mf6 runs.  Open the \"helpers.py\" script in the notebooks/ directory and find the \"process_csv_files()\" functiuon.  What does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.process_csv_files(model_ws=pf.new_d)\n",
    "pf.add_py_function(\"helpers.py\", \"process_csv_files()\", is_pre_cmd=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Tell `PstFrom` to run mf6 as the \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.mod_sys_cmds.append(\"mf6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Add the first set of model outputs as \"observations\" in the pest interface: \"swgw-longterm-means.csv\".  This csv contains the simulated equivalents to the predictions we are most concerned with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(pf.new_d, \"swgw-longterm-means.csv\"), index_col=0)\n",
    "df = pf.add_observations(\n",
    "    \"swgw-longterm-means.csv\",\n",
    "    index_cols=\"quantity\",\n",
    "    prefix=\"forecasts\",\n",
    "    obsgp=\"forecasts\",\n",
    "    ofile_sep=\",\",\n",
    ")\n",
    "print(df.index.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Check your working directory to ensure that an instruction file was created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Now lets gather up all the output timeseries csv files we want to have as observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_csv_files = [\n",
    "    f for f in os.listdir(pf.new_d) if f.startswith(\"sv.gwf\") and f.endswith(\".csv\")\n",
    "]\n",
    "obs_csv_files.extend(\n",
    "    [f for f in os.listdir(pf.new_d) if f.startswith(\"sv.lake\") and f.endswith(\".csv\")]\n",
    ")\n",
    "obs_csv_files.extend(\n",
    "    [f for f in os.listdir(pf.new_d) if f.startswith(\"sv.riv\") and f.endswith(\".csv\")]\n",
    ")\n",
    "obs_csv_files.extend(\n",
    "    [f for f in os.listdir(pf.new_d) if f.startswith(\"sv.sfr\") and f.endswith(\".csv\")]\n",
    ")\n",
    "obs_csv_files.extend(\n",
    "    [\n",
    "        f\n",
    "        for f in os.listdir(pf.new_d)\n",
    "        if f.startswith(\"sv.spring\") and f.endswith(\".csv\")\n",
    "    ]\n",
    ")\n",
    "obs_csv_files.extend(\n",
    "    [\n",
    "        f\n",
    "        for f in os.listdir(pf.new_d)\n",
    "        if f.startswith(\"sv-budget\") and f.endswith(\".csv\")\n",
    "    ]\n",
    ")\n",
    "obs_csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Loop over them and add each one to the interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs_csv_file in obs_csv_files:\n",
    "    print(obs_csv_file)\n",
    "    prefix = obs_csv_file.replace(\".\", \"-\")\n",
    "    df = pd.read_csv(os.path.join(pf.new_d, obs_csv_file), index_col=0)\n",
    "    odf = pf.add_observations(\n",
    "        obs_csv_file,\n",
    "        index_cols=\"datetime\",\n",
    "        use_cols=df.columns.to_list(),\n",
    "        prefix=prefix,\n",
    "        ofile_sep=\",\",\n",
    "    )\n",
    "    print(odf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Now some parameters.  Start with hk - the ole classic.  Find all of the HK input arrays that mf6 using.  These files contain the string \"sv.npf_k_layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_files = [f for f in os.listdir(pf.new_d) if f.startswith(\"sv.npf_k_layer\")]\n",
    "assert len(k_files) == gwf.dis.nlay.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_files.sort()\n",
    "k_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "We need to define some spatial correlation functions/information for the pilot points (for both interpolation from pilot points to the grid and also for the Prior covariance).  We will use a different correlation function for each property type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_v_k = pyemu.geostats.ExpVario(contribution=1.0, a=10000)\n",
    "pp_geostruct_k = pyemu.geostats.GeoStruct(variograms=pp_v_k, transform=\"log\")\n",
    "pp_geostruct_k.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Now define variograms and geostructs for k33, ss, and sy using the same naming scheme:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_v_k33 = pyemu.geostats.ExpVario(contribution=1.0, a=5000)\n",
    "pp_geostruct_k33 = pyemu.geostats.GeoStruct(variograms=pp_v_k33, transform=\"log\")\n",
    "pp_v_ss = pyemu.geostats.ExpVario(contribution=1.0, a=15000)\n",
    "pp_geostruct_ss = pyemu.geostats.GeoStruct(variograms=pp_v_ss, transform=\"log\")\n",
    "pp_v_sy = pyemu.geostats.ExpVario(contribution=1.0, a=7000)\n",
    "pp_geostruct_sy = pyemu.geostats.GeoStruct(variograms=pp_v_sy, transform=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "We will treat HK in layer 1 and 2 as same quantity - they will share pilot point multiplier parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pf.add_parameters(\n",
    "    k_files[:2],\n",
    "    par_type=\"pilotpoints\",\n",
    "    pp_options={\"pp_space\": 3},\n",
    "    lower_bound=0.1,\n",
    "    upper_bound=10.0,\n",
    "    geostruct=pp_geostruct_k,\n",
    "    par_name_base=\"hk-pp-wt\",\n",
    "    pargp=\"hk-pp-wt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Notice the bound information being passed - this will be used to define the prior distrbution later..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Since the pilot points are designed to accomodate spatial heterogeneity, let's also include a layer-constant parameter to help sample a wider range of HK values.  Tag this parameter with \"hk-cn-wt\" for \"constant HK in the water table aquifer\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pf.add_parameters(\n",
    "    k_files[:2],\n",
    "    par_type=\"constant\",\n",
    "    lower_bound=0.1,\n",
    "    upper_bound=10.0,\n",
    "    par_name_base=\"hk-cn-wt\",\n",
    "    pargp=\"hk-cn-wt\",\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Do the same for HK in layers 4 and 5 together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pf.add_parameters(\n",
    "    k_files[3:],\n",
    "    par_type=\"pilotpoints\",\n",
    "    pp_options={\"pp_space\": 3},\n",
    "    lower_bound=0.1,\n",
    "    upper_bound=10.0,\n",
    "    geostruct=pp_geostruct_k,\n",
    "    par_name_base=\"hk-pp-aq\",\n",
    "    pargp=\"hk-pp-aq\",\n",
    ")\n",
    "df = pf.add_parameters(\n",
    "    k_files[3:],\n",
    "    par_type=\"constant\",\n",
    "    lower_bound=0.10,\n",
    "    upper_bound=1.0,\n",
    "    par_name_base=\"hk-cn-aq\",\n",
    "    pargp=\"hk-cn-aq\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "To let us see what the actual HK array that mf6 sees, let's add that array as a set of observatitons also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_file in [k_files[0], k_files[-1]]:\n",
    "    print(k_file)\n",
    "    pf.add_observations(\n",
    "        k_file,\n",
    "        obsgp=k_file.split(\".\")[1].replace(\"_\", \"-\"),\n",
    "        prefix=k_file.split(\".\")[1].replace(\"_\", \"-\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Setup a similar scheme of parameters to K33.  First find all of the K33 array files (tagged with \"\"sv.npf_k33_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "k33_files = [f for f in os.listdir(pf.new_d) if f.startswith(\"sv.npf_k33_layer\")]\n",
    "assert len(k_files) == gwf.dis.nlay.data\n",
    "k33_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Now setup pilot points and constant parameters for these arrays in a similar way that we did for K, but focusing on K33 of model layer 3 (the semi-confining unit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pf.add_parameters(\n",
    "    k33_files[2],\n",
    "    par_type=\"pilotpoints\",\n",
    "    pp_options={\"pp_space\": 3},\n",
    "    lower_bound=0.01,\n",
    "    upper_bound=100.0,\n",
    "    geostruct=pp_geostruct_k33,\n",
    "    par_name_base=\"k33-pp-conf\",\n",
    "    pargp=\"k33-pp-conf\",\n",
    ")\n",
    "df = pf.add_parameters(\n",
    "    k33_files[2],\n",
    "    par_type=\"constant\",\n",
    "    lower_bound=0.1,\n",
    "    upper_bound=10.0,\n",
    "    par_name_base=\"k33-cn-conf\",\n",
    "    pargp=\"k33-cn-conf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Now add the layer 3 K33 array as observations so we can monitor those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.add_observations(\n",
    "    k33_files[2],\n",
    "    obsgp=k33_files[2].split(\".\")[1].replace(\"_\", \"-\"),\n",
    "    prefix=k33_files[2].split(\".\")[1].replace(\"_\", \"-\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "And SS and sy (in layer 1 only).  Same as before: find the SS arrays (tagged with \"sv.sto_ss\")\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_files = [f for f in os.listdir(pf.new_d) if f.startswith(\"sv.sto_ss\")]\n",
    "assert len(ss_files) == gwf.dis.nlay.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "Add SS pilot points and constants for each model layer.  Use an upper bound and lower bound that give us lots of flexiibility to fit data...also add those SS arrays as observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pf.add_parameters(\n",
    "    ss_files[:2],\n",
    "    par_type=\"pilotpoints\",\n",
    "    pp_options={\"pp_space\": 3},\n",
    "    lower_bound=0.05,\n",
    "    upper_bound=20.0,\n",
    "    geostruct=pp_geostruct_ss,\n",
    "    par_name_base=\"ss-pp-wt\",\n",
    "    pargp=\"ss-pp-wt\",\n",
    ")\n",
    "df = pf.add_parameters(\n",
    "    ss_files[:2],\n",
    "    par_type=\"constant\",\n",
    "    lower_bound=0.1,\n",
    "    upper_bound=10.0,\n",
    "    par_name_base=\"ss-cn-wt\",\n",
    "    pargp=\"ss-cn-wt\",\n",
    ")\n",
    "df = pf.add_parameters(\n",
    "    ss_files[2],\n",
    "    par_type=\"pilotpoints\",\n",
    "    pp_options={\"pp_space\": 3},\n",
    "    lower_bound=0.05,\n",
    "    upper_bound=20.0,\n",
    "    geostruct=pp_geostruct_ss,\n",
    "    par_name_base=\"ss-pp-conf\",\n",
    "    pargp=\"ss-pp-conf\",\n",
    ")\n",
    "df = pf.add_parameters(\n",
    "    ss_files[2],\n",
    "    par_type=\"constant\",\n",
    "    lower_bound=0.05,\n",
    "    upper_bound=20.0,\n",
    "    par_name_base=\"ss-cn-conf\",\n",
    "    pargp=\"ss-cn-conf\",\n",
    ")\n",
    "df = pf.add_parameters(\n",
    "    ss_files[3:],\n",
    "    par_type=\"pilotpoints\",\n",
    "    pp_options={\"pp_space\": 3},\n",
    "    lower_bound=0.05,\n",
    "    upper_bound=20.0,\n",
    "    geostruct=pp_geostruct_ss,\n",
    "    par_name_base=\"ss-pp-aq\",\n",
    "    pargp=\"ss-pp-aq\",\n",
    ")\n",
    "df = pf.add_parameters(\n",
    "    ss_files[3:],\n",
    "    par_type=\"constant\",\n",
    "    lower_bound=0.1,\n",
    "    upper_bound=10.0,\n",
    "    par_name_base=\"ss-cn-aq\",\n",
    "    pargp=\"ss-cn-aq\",\n",
    ")\n",
    "\n",
    "pf.add_observations(\n",
    "    ss_files[0],\n",
    "    obsgp=ss_files[0].split(\".\")[1].replace(\"_\", \"-\"),\n",
    "    prefix=ss_files[0].split(\".\")[1].replace(\"_\", \"-\"),\n",
    ")\n",
    "\n",
    "pf.add_observations(\n",
    "    ss_files[2],\n",
    "    obsgp=ss_files[2].split(\".\")[1].replace(\"_\", \"-\"),\n",
    "    prefix=ss_files[2].split(\".\")[1].replace(\"_\", \"-\"),\n",
    ")\n",
    "\n",
    "pf.add_observations(\n",
    "    ss_files[-1],\n",
    "    obsgp=ss_files[-1].split(\".\")[1].replace(\"_\", \"-\"),\n",
    "    prefix=ss_files[-1].split(\".\")[1].replace(\"_\", \"-\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Now sy - just in layer 1 tho.  and we need to be more convervative with the parameter bounds so that we dont get unrealistically high sy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sy_files = [f for f in os.listdir(pf.new_d) if f.startswith(\"sv.sto_sy\")]\n",
    "assert len(sy_files) == gwf.dis.nlay.data\n",
    "sy_files.sort()\n",
    "sy_file = sy_files[0]\n",
    "assert \"layer1\" in sy_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pf.add_parameters(\n",
    "    sy_file,\n",
    "    par_type=\"pilotpoints\",\n",
    "    pp_options={\"pp_space\": 3},\n",
    "    lower_bound=0.6,\n",
    "    upper_bound=1.4,\n",
    "    geostruct=pp_geostruct_sy,\n",
    "    par_name_base=\"sy-pp-wt\",\n",
    "    pargp=\"sy-pp-wt\",\n",
    "    ult_ubound=1.0,\n",
    "    transform=\"none\",\n",
    ")\n",
    "df = pf.add_parameters(\n",
    "    sy_file,\n",
    "    par_type=\"constant\",\n",
    "    lower_bound=0.9,\n",
    "    upper_bound=1.1,\n",
    "    par_name_base=\"sy-cn-wt\",\n",
    "    pargp=\"sy-cn-wt\",\n",
    "    transform=\"none\",\n",
    ")\n",
    "pf.add_observations(\n",
    "    sy_file,\n",
    "    obsgp=sy_file.split(\".\")[1].replace(\"_\", \"-\"),\n",
    "    prefix=sy_file.split(\".\")[1].replace(\"_\", \"-\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Set up some parameters for the pumping wells - we arent going to adjust these (dont we all have perfect historic water use data?!), but we will use them as decision variables later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "Now find any remaining wel/maw files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_files = [f for f in os.listdir(pf.new_d) if f.startswith(\"sv.wel_stress\")]\n",
    "assert len(wel_files) > 0\n",
    "wel_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "And add parameters for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wel_file in wel_files:\n",
    "    kper = int(wel_file.split(\".\")[1].split(\"_\")[-1]) - 1\n",
    "    if kper == 0:\n",
    "        continue\n",
    "    pf.add_parameters(\n",
    "        wel_file,\n",
    "        par_type=\"grid\",\n",
    "        par_style=\"m\",\n",
    "        par_name_base=\"welrate_kper:{0}\".format(kper),\n",
    "        pargp=\"welrate_kper:{0}\".format(kper),\n",
    "        mfile_skip=0,\n",
    "        index_cols=[0, 1, 2],\n",
    "        use_cols=[3],\n",
    "        mfile_fmt=\"free\",\n",
    "        upper_bound=3.0,\n",
    "        lower_bound=0.0,\n",
    "        transform=\"none\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "Add some recharge parameters for the base model - this is to try to account for the uncertainty that has been introduced through simplification...if we are using uzf, then add some small uncertainties for precip/infilt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rech_files = [\n",
    "    f for f in os.listdir(pf.new_d) if f.startswith(\"sv.rch_stress_period_data_\")\n",
    "]\n",
    "\n",
    "\n",
    "assert len(rech_files) == sim.tdis.nper.data\n",
    "rech_files.sort()\n",
    "df = pf.add_parameters(\n",
    "    rech_files,\n",
    "    par_type=\"constant\",\n",
    "    lower_bound=0.9,\n",
    "    upper_bound=1.1,\n",
    "    par_name_base=\"rech_global\",\n",
    "    pargp=\"rech_global\",\n",
    "    transform=\"none\",\n",
    "    index_cols=[0, 1, 2],\n",
    "    use_cols=[3],\n",
    "    mfile_skip=0,\n",
    ")\n",
    "for rech_file in rech_files:\n",
    "    kper = int(rech_file.split(\".\")[1].split(\"_\")[-1]) - 1\n",
    "    print(rech_file)\n",
    "    df = pf.add_parameters(\n",
    "        rech_file,\n",
    "        par_type=\"constant\",\n",
    "        lower_bound=0.7,\n",
    "        upper_bound=1.3,\n",
    "        par_name_base=\"rech\",\n",
    "        pargp=\"rech\",\n",
    "        transform=\"none\",\n",
    "        index_cols=[0, 1, 2],\n",
    "        use_cols=[3],\n",
    "        mfile_skip=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_file = \"sv.sfr_packagedata.txt\"\n",
    "if os.path.exists(os.path.join(pf.new_d, sfr_file)):\n",
    "    pf.add_parameters(\n",
    "        sfr_file,\n",
    "        index_cols=[0, 1, 2],\n",
    "        use_cols=[9],\n",
    "        pargp=\"sfrhk\",\n",
    "        par_name_base=\"sfrhk\",\n",
    "        lower_bound=0.1,\n",
    "        upper_bound=10,\n",
    "        par_type=\"constant\",\n",
    "        mfile_skip=0,\n",
    "    )\n",
    "else:\n",
    "    riv_files = [f for f in os.listdir(pf.new_d) if \"riv_stress\" in f]\n",
    "    print(riv_files)\n",
    "    pf.add_parameters(\n",
    "        riv_files,\n",
    "        index_cols=[0, 1, 2],\n",
    "        use_cols=[4],\n",
    "        pargp=\"rivcond\",\n",
    "        par_name_base=\"rivcond\",\n",
    "        lower_bound=0.1,\n",
    "        upper_bound=10,\n",
    "        par_type=\"constant\",\n",
    "        mfile_skip=0,\n",
    "    )\n",
    "    pf.add_parameters(\n",
    "        riv_files,\n",
    "        index_cols=[0, 1, 2],\n",
    "        use_cols=[5],\n",
    "        pargp=\"rivstage\",\n",
    "        par_name_base=\"rivstage\",\n",
    "        lower_bound=-0.5,\n",
    "        upper_bound=0.5,\n",
    "        par_type=\"constant\",\n",
    "        mfile_skip=0,\n",
    "        par_style=\"a\",\n",
    "        transform=\"none\",\n",
    "        initial_value=0.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "Now build the interface and the control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.build_pst(filename=\"pest.pst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "Go to the `working_d` and see what has happened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Check the `obsval` quantities in the \"* observation data\" section - what are those numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pf.pst.observation_data\n",
    "obs.obsval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "So if the `obsval` values are all the existing model output values, then if we run the model again just the same way, we should have a phi of zero - a great check!. Let's do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.pst.control_data.noptmax = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.pst.write(os.path.join(pf.new_d, \"pest.pst\"), version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "Use the `pyemu.os_utils.run()` function to run pestpp-ies with our new control file in the `working_d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-ies pest.pst\", cwd=pf.new_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "Use the `Pst.set_res()` method to point to the \"pest.base.rei\" file and check the phi value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.pst.set_res(os.path.join(pf.new_d, \"pest.base.rei\"))\n",
    "pf.pst.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pf.pst.phi < 1.0e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "now find the subset of parameters that have \"wel\" in the parnme (ie the name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pf.pst.parameter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "wellpars = par.loc[par.parnme.str.contains(\"wel\"), :]\n",
    "assert len(wellpars) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "Mark there \"partrans\" as \"fixed\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "par.loc[wellpars.parnme, \"partrans\"] = \"fixed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "Now generate a Prior parameter ensemble (which the parameter bound and geostat info we passed to `PstFrom` above) via the `PstFrom.draw()` method.   Generate 1000 realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pf.draw(num_reals=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "We need to enforce parameter bounds on those realizations, save it and add an arg to the control file to tell ies to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.enforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "Now save the parameter ensemble to a file in the `working_d` and tell pestpp-ies to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.to_csv(os.path.join(pf.new_d, \"prior.csv\"))\n",
    "pf.pst.pestpp_options[\"ies_par_en\"] = \"prior.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "Save the control file one more time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.pst.write(os.path.join(pf.new_d, \"pest.pst\"), version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "## Set observation values, weights and noise\n",
    "\n",
    "So far, the control file only has simulated outputs and weights of 1 for everything.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "Load the actual \"observation data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_csv_fname = os.path.join(\n",
    "#     \"..\", \"models\", \"synthetic-valley-truth-advanced-monthly\", \"raw_obs.csv\"\n",
    "# )\n",
    "# assert os.path.exists(obs_csv_fname)\n",
    "# obsdf = pd.read_csv(obs_csv_fname, index_col=0, parse_dates=True)\n",
    "# obsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "Models are always going to be low-pass filters compared to the complex natural systems that generated the observations.  So its usually a good idea to filter out high freq signal components.  Apply a rolling mean to each timeseries using the `window` parameter we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothed = obsdf.rolling(window=window, center=True, min_periods=1).mean()\n",
    "# for col in smoothed.columns:\n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "#     smoothed.loc[smoothed.index.year < 2015, col].plot(ax=ax, c=\"g\")\n",
    "\n",
    "#     obsdf.loc[obsdf.index.year < 2015, col].plot(ax=ax, c=\"m\")\n",
    "\n",
    "#     ax.set_title(col, loc=\"left\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothedlow = obsdf.rolling(window=window, center=True, min_periods=1).quantile(0.65)\n",
    "# for col in smoothedlow.columns:\n",
    "#     if \"riv-flow\" not in col:\n",
    "#         continue\n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "#     smoothedlow.loc[smoothedlow.index.year < 2015, col].plot(ax=ax, c=\"g\")\n",
    "\n",
    "#     obsdf.loc[obsdf.index.year < 2015, col].plot(ax=ax, c=\"m\")\n",
    "\n",
    "#     ax.set_title(col, loc=\"left\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothed[\"riv-flow\"] = smoothedlow[\"riv-flow\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "Now load the control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pst = pyemu.Pst(os.path.join(working_d, \"pest.pst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = pst.observation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "Now for the tricky part: we need to find each simulated output that we have an observed counterpart for.  In practice, this usually requires some bespoke code/hackery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnobs = obs.loc[pd.notna(obs.usecol), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefixes = [\n",
    "#     \"wt\",\n",
    "#     \"aq\",\n",
    "#     \"lake-stage\",\n",
    "#     \"lake-swgw\",\n",
    "#     \"riv-flow\",\n",
    "#     \"riv-swgw\",\n",
    "#     \"diff1\",\n",
    "#     \"diff0\",\n",
    "# ]\n",
    "# for prefix in prefixes:\n",
    "#     uobs = nnobs.loc[nnobs.usecol.str.contains(prefix), :].copy()\n",
    "#     print(prefix, uobs.shape)\n",
    "#     uobs[\"datetime\"] = pd.to_datetime(uobs.datetime)\n",
    "#     for usecol in uobs.usecol.unique():\n",
    "#         print(usecol)\n",
    "#         uuobs = uobs.loc[uobs.usecol == usecol, :].copy()\n",
    "#         for dt, name in zip(uuobs.datetime, uuobs.obsnme):\n",
    "#             oval = smoothed.loc[dt, usecol]\n",
    "#             obs.loc[name, \"obsval\"] = oval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "now we need to set the weights and expected noise for each observation datum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pf.pst.observation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs[\"weight\"] = 0.0\n",
    "obs[\"standard_deviation\"] = np.nan\n",
    "obs[\"lower_bound\"] = np.nan\n",
    "obs[\"upper_bound\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "In this block, the weights and noise for each observation is defined...do you agree with these values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_dict = {}\n",
    "# hist_prefixes = [\"wt\", \"aq\", \"lake-stage\", \"diff1\", \"diff0\"]\n",
    "# for prefix in hist_prefixes:\n",
    "#     uobs = nnobs.loc[nnobs.usecol.str.startswith(prefix), :].copy()\n",
    "#     print(uobs.usecol.unique())\n",
    "#     uobs[\"datetime\"] = pd.to_datetime(uobs.datetime)\n",
    "#     hist_uobs = uobs.loc[uobs.datetime.dt.year < 2015, :]\n",
    "#     obs.loc[hist_uobs.obsnme, \"datetime\"] = hist_uobs.datetime\n",
    "#     if \"lake\" in prefix:\n",
    "#         obs.loc[hist_uobs.obsnme, \"weight\"] = 5.0\n",
    "#         obs.loc[hist_uobs.obsnme, \"standard_deviation\"] = 0.2\n",
    "#     elif \"diff\" in prefix:\n",
    "#         print(prefix)\n",
    "#         obs.loc[hist_uobs.obsnme, \"weight\"] = [\n",
    "#             3.0 if oval > 0.1 else 3.0 for oval in np.abs(hist_uobs.obsval)\n",
    "#         ]\n",
    "#         obs.loc[hist_uobs.obsnme, \"standard_deviation\"] = [\n",
    "#             max(0.01, oval * 0.25) for oval in np.abs(hist_uobs.obsval)\n",
    "#         ]\n",
    "#         obs.loc[hist_uobs.obsnme, \"lower_bound\"] = 0.0\n",
    "#     else:\n",
    "#         obs.loc[hist_uobs.obsnme, \"weight\"] = 2.0\n",
    "#         obs.loc[hist_uobs.obsnme, \"standard_deviation\"] = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "if this is an advanced model, we can also use riv-flow information for history matching (one benefit of a more complex model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"riv-flow\" in obs.usecol.unique():\n",
    "#     uobs = nnobs.loc[nnobs.usecol == \"riv-flow\", :].copy()\n",
    "#     uobs[\"datetime\"] = pd.to_datetime(uobs.datetime)\n",
    "#     hist_uobs = uobs.loc[uobs.datetime.dt.year < 2015, :].copy()\n",
    "#     hist_uobs[\"standard_deviation\"] = [\n",
    "#         max(0.2, oval * 0.2) for oval in np.abs(hist_uobs.obsval.values)\n",
    "#     ]\n",
    "#     hist_uobs.loc[hist_uobs.obsnme, \"weight\"] = 1 / hist_uobs.standard_deviation.values\n",
    "#     obs.loc[hist_uobs.obsnme, \"standard_deviation\"] = (\n",
    "#         hist_uobs.standard_deviation.values\n",
    "#     )\n",
    "#     obs.loc[hist_uobs.obsnme, \"weight\"] = hist_uobs.weight.values\n",
    "#     obs.loc[hist_uobs.obsnme, \"datetime\"] = hist_uobs.datetime\n",
    "\n",
    "#     print(hist_uobs.loc[:, [\"obsval\", \"standard_deviation\", \"weight\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "Remember that we dont want sy to get too large?  Let's tell ies about that.  First find all observations with \"sto-sy\" in the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syobs = obs.loc[obs.obsnme.str.contains(\"sto-sy\"), :]\n",
    "# syobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "Now give them all a `weight` of 1, an `obgnme` of \"less_than_sy\", and an `obsval` of 0.3.  this will setup inequality observations to (try to) keep sy less than 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs.loc[syobs.obsnme, \"weight\"] = 1.0\n",
    "# obs.loc[syobs.obsnme, \"obgnme\"] = \"less_than_sy\"\n",
    "# obs.loc[syobs.obsnme, \"obsval\"] = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pst.nnz_obs_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.final_steps(pst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "Set noptmax to 0, save the control file and do a test run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.pst.control_data.noptmax = 0\n",
    "pf.pst.write(os.path.join(working_d, \"pest.pst\"), version=2)\n",
    "pyemu.os_utils.run(\"pestpp-ies pest.pst\", cwd=working_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "Now we are going to generate some autocorrelated timeseries noise to use in the history matching.  We are going to use a (very) long correlation lenght to express that we are more interested in low-frequency noise/error than high frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_length_days = 365 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nzobs = obs.loc[obs.weight > 0, :].copy()\n",
    "# obs[\"distance\"] = np.nan\n",
    "# grps = nzobs.obgnme.unique()\n",
    "# grps.sort()\n",
    "# struct_dict = {}\n",
    "# for grp in grps:\n",
    "#     if \"less_than_sy\" in grp:\n",
    "#         continue\n",
    "#     gobs = nzobs.loc[nzobs.obgnme == grp, :].copy()\n",
    "#     gobs[\"datetime\"] = pd.to_datetime(gobs.datetime)\n",
    "#     gobs[\"distance\"] = (gobs.datetime - gobs.datetime.min()).dt.days\n",
    "#     obs.loc[gobs.obsnme, \"distance\"] = gobs.distance\n",
    "#     v = pyemu.geostats.ExpVario(contribution=1.0, a=autocorrelation_length_days)\n",
    "#     gs = pyemu.geostats.GeoStruct(variograms=v, name=grp)\n",
    "#     names = gobs.obsnme.to_list()\n",
    "#     names.sort()\n",
    "#     struct_dict[gs] = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# struct_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "Set a random seed to make sure we are getting the same draws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(pyemu.en.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "Generate an obs+noise ensemble using `pyemu.helpers.autocorrelated_draw()`.  Draw 1000 realizations and enforce bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = pyemu.helpers.autocorrelated_draw(\n",
    "#     pst, struct_dict, num_reals=1000, verbose=True, enforce_bounds=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "Explore the `noise` ensemble:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise.loc[:, pst.nnz_obs_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wtobs = obs.loc[obs.obsnme.str.contains(\"wt\"),:]\n",
    "# usecols = wtobs.usecol.unique()\n",
    "# usecols.sort()\n",
    "# for usecol in usecols:\n",
    "#     wwtobs = wtobs.loc[wtobs.usecol==usecol,:]\n",
    "#     usecol = usecol.replace(\"wt\",\"aq\")\n",
    "#     aqobs = obs.loc[obs.usecol==usecol,:]\n",
    "#     assert aqobs.shape[0] > 0\n",
    "#     usecol = usecol.replace(\"aq\",\"diff\")\n",
    "#     dfobs = obs.loc[obs.usecol==usecol,:]\n",
    "#     assert dfobs.shape[0] > 0\n",
    "#     #print(noise.loc[:,dfobs.obsnme])\n",
    "#     noise.loc[:,dfobs.obsnme] = noise.loc[:,wwtobs.obsnme].values - noise.loc[:,aqobs.obsnme].values\n",
    "#     #print(noise.loc[:,dfobs.obsnme])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "Save the noise ensemble, tell ies about it, and do an noptmax=-2 test run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise.to_csv(os.path.join(working_d, \"noise.csv\"))\n",
    "# pst.pestpp_options = {\"ies_par_en\": pst.pestpp_options[\"ies_par_en\"]}\n",
    "# pst.pestpp_options[\"ies_obs_en\"] = \"noise.csv\"\n",
    "# pst.control_data.noptmax = -2\n",
    "# pst.write(os.path.join(working_d, \"pest.pst\"), version=2)\n",
    "# pyemu.os_utils.run(\"pestpp-ies pest.pst\", cwd=working_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "As you can see from the phi group summary, we need some rebalanced weights.  One way to do this is the the ies_phi_factor_file.  First define a `dict` of string-tags:phi fractions we want:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.plot_ies_timeseries(working_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi_factors = {\"lake\": 0.15, \"aq\": 0.3, \"wt\": 0.3, \"diff\": 0.25, \"less_than_sy\": -999}\n",
    "# if \"riv-flow\" in obs.usecol.unique():\n",
    "#     phi_factors = {\n",
    "#         \"lake\": 0.1,\n",
    "#         \"aq\": 0.3,\n",
    "#         \"wt\": 0.3,\n",
    "#         \"diff\": 0.2,\n",
    "#         \"riv-flow\": 0.1,\n",
    "#         \"less_than_sy\": -999,\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "Write this information to a two-column csv file (no header!) in the `working_d`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ser = pd.Series(phi_factors)\n",
    "# ser.to_csv(os.path.join(working_d, \"phi_facs.csv\"), index=True, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "Now tell pestpp-ies to use it, re-write the control file and run pestpp-ies again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pst.pestpp_options[\"ies_phi_factor_file\"] = \"phi_facs.csv\"\n",
    "# pst.write(os.path.join(working_d, \"pest.pst\"), version=2)\n",
    "# pyemu.os_utils.run(\"pestpp-ies pest.pst\", cwd=working_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_prior_mc:\n",
    "#     master_d = \"master_prior_mc\"\n",
    "#     pst.pestpp_options[\"ies_num_reals\"] = 50\n",
    "#     pst.control_data.noptmax = -1\n",
    "#     pst.write(os.path.join(working_d, \"pest.pst\"), version=2)\n",
    "#     pyemu.os_utils.start_workers(\n",
    "#         pf.new_d,\n",
    "#         \"pestpp-ies\",\n",
    "#         \"pest.pst\",\n",
    "#         worker_root=\".\",\n",
    "#         num_workers=num_workers,\n",
    "#         master_dir=master_d,\n",
    "#     )\n",
    "#     pst = pyemu.Pst(os.path.join(master_d, \"pest.pst\"))\n",
    "#     helpers.plot_ies_forecasts(master_d)\n",
    "#     helpers.plot_ies_timeseries(master_d)\n",
    "#     tags = [\n",
    "#         \"npf-k-layer1\",\n",
    "#         \"npf-k-layer5\",\n",
    "#         \"npf-k33-layer3\",\n",
    "#         \"sto-ss-layer1\",\n",
    "#         \"sto-sy-layer1\",\n",
    "#     ]\n",
    "#     for tag in tags:\n",
    "#         helpers.plot_ies_properties(master_d, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
